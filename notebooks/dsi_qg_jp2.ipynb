{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=11.0 -c pytorch\n",
    "# pip install ipywidgets --upgrade\n",
    "# pip install jupyter --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of example (all is query) in test_data: 862\n",
      "Number of example (all is query) in dev_data: 1000\n",
      "Number of example in train_data: 35609\n",
      "Number of document in train data: 18304\n",
      "Number of query in train data: 17304\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi file JSONL\n",
    "test_path = \"/home/hoang/DSI-QG/data/msmarco/test_data_dsi.json\"\n",
    "dev_path = \"/home/hoang/DSI-QG/data/msmarco/validation_data_dsi.json\"\n",
    "train_path = \"/home/hoang/DSI-QG/data/msmarco/train_data_dsi.json\"\n",
    "\n",
    "# M·ªü file v√† ƒë·ªçc t·ª´ng d√≤ng\n",
    "with open(test_path, 'r', encoding='utf-8') as file:\n",
    "    test_data = [json.loads(line) for line in file]\n",
    "print(\"Number of example (all is query) in test_data:\", len(test_data))\n",
    "\n",
    "with open(dev_path, 'r', encoding='utf-8') as file:\n",
    "    dev_data = [json.loads(line) for line in file]\n",
    "print(\"Number of example (all is query) in dev_data:\", len(dev_data))\n",
    "\n",
    "with open(train_path, 'r', encoding='utf-8') as file:\n",
    "    train_data = [json.loads(line) for line in file]\n",
    "print(\"Number of example in train_data:\", len(train_data))\n",
    "print(\"Number of document in train data:\", (len(train_data) - len(dev_data)) // 2 + len(dev_data))\n",
    "print(\"Number of query in train data:\", (len(train_data) - len(dev_data)) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Original DSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoang/DSI-QG\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-be889588cf0a851e\n",
      "Downloading and preparing dataset json/default to cache/json/default-be889588cf0a851e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3751.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 512.75it/s]\n",
      "Dataset json downloaded and prepared to cache/json/default-be889588cf0a851e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 26.14it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18000/18000 [00:01<00:00, 15459.78it/s]\n",
      "Using custom data configuration default-ccc010dd9134ceca\n",
      "Downloading and preparing dataset json/default to cache/json/default-ccc010dd9134ceca/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4301.85it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 919.40it/s]\n",
      "Dataset json downloaded and prepared to cache/json/default-ccc010dd9134ceca/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 303.10it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 14877.45it/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 582401280\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrunghoang\u001b[0m (\u001b[33mhoangtrung\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/hoang/DSI-QG/wandb/run-20241118_033312-wismtfim\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmsmarco-mt5-base-DSI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtrung/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtrung/huggingface/runs/wismtfim\u001b[0m\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1/10 [00:04<00:36,  4.06s/it]Traceback (most recent call last):\n",
      "  File \"run.py\", line 200, in <module>\n",
      "    main()\n",
      "  File \"run.py\", line 165, in main\n",
      "    trainer.train()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 2557, in training_step\n",
      "    loss.backward()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/tensor.py\", line 221, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 130, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 23.65 GiB total capacity; 10.12 GiB already allocated; 145.31 MiB free; 10.41 GiB reserved in total by PyTorch)\n",
      "Traceback (most recent call last):\n",
      "  File \"run.py\", line 200, in <module>\n",
      "    main()\n",
      "  File \"run.py\", line 165, in main\n",
      "    trainer.train()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 2557, in training_step\n",
      "    loss.backward()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/tensor.py\", line 221, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 130, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 23.65 GiB total capacity; 10.12 GiB already allocated; 145.31 MiB free; 10.41 GiB reserved in total by PyTorch)\n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mmsmarco-mt5-base-DSI\u001b[0m at: \u001b[34mhttps://wandb.ai/hoangtrung/huggingface/runs/wismtfim\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241118_033312-wismtfim/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=1 python run.py \\\n",
    "        --task \"DSI\" \\\n",
    "        --model_name \"google/mt5-base\" \\\n",
    "        --run_name \"msmarco-mt5-base-DSI\" \\\n",
    "        --max_length 256 \\\n",
    "        --train_file data/msmarco/train_data_dsi.json \\\n",
    "        --valid_file data/msmarco/test_data_dsi.json \\\n",
    "        --output_dir \"models/msmarco-mt5-base-DSI\" \\\n",
    "        --learning_rate 0.0005 \\\n",
    "        --warmup_steps 2 \\\n",
    "        --per_device_train_batch_size 2 \\\n",
    "        --per_device_eval_batch_size 2 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 5 \\\n",
    "        --max_steps 10 \\\n",
    "        --save_strategy steps \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --save_steps 1000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --load_best_model_at_end \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --dataloader_drop_last False \\\n",
    "        --metric_for_best_model Hits@10 \\\n",
    "        --greater_is_better True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --gpu-reset\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=0 python run.py \\\n",
    "        --task \"DSI\" \\\n",
    "        --model_name \"google/mt5-base\" \\\n",
    "        --run_name \"msmarco-100k-mt5-base-DSI\" \\\n",
    "        --max_length 256 \\\n",
    "        --train_file data/msmarco_data/100k/msmarco_DSI_train_data.json \\\n",
    "        --valid_file data/msmarco_data/100k/msmarco_DSI_dev_data.json \\\n",
    "        --output_dir \"models/msmarco-100k-mt5-base-DSI\" \\\n",
    "        --learning_rate 0.0005 \\\n",
    "        --warmup_steps 100 \\\n",
    "        --per_device_train_batch_size 8 \\\n",
    "        --per_device_eval_batch_size 8 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 1000 \\\n",
    "        --max_steps 2000 \\\n",
    "        --save_strategy steps \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --save_steps 1000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --load_best_model_at_end \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --dataloader_drop_last False \\\n",
    "        --metric_for_best_model Hits@10 \\\n",
    "        --greater_is_better True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DSI-QG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train a query generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=1 python run.py \\\n",
    "        --task \"docTquery\" \\\n",
    "        --model_name \"google/mt5-large\" \\\n",
    "        --run_name \"docTquery-MSMARCO\" \\\n",
    "        --max_length 128 \\\n",
    "        --train_file data/msmarco_data/100k/msmarco_DSI_train_data.json \\\n",
    "        --valid_file data/msmarco_data/100k/msmarco_DSI_dev_data.json \\\n",
    "        --output_dir \"models/msmarco_docTquery_mt5_large\" \\\n",
    "        --learning_rate 0.0001 \\\n",
    "        --warmup_steps 0 \\\n",
    "        --per_device_train_batch_size 4 \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 100 \\\n",
    "        --max_steps 2000 \\\n",
    "        --save_strategy steps \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --save_steps 100 \\\n",
    "        --save_total_limit 2 \\\n",
    "        --load_best_model_at_end \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --dataloader_drop_last False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the query generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=1 python run.py \\\n",
    "        --task generation \\\n",
    "        --model_name google/mt5-large \\\n",
    "        --model_path models/msmarco_docTquery_mt5_large/checkpoint-xxx \\\n",
    "        --per_device_eval_batch_size 32 \\\n",
    "        --run_name docTquery-MSMARCO-generation \\\n",
    "        --max_length 256 \\\n",
    "        --valid_file data/msmarco_data/100k/msmarco_corpus.tsv \\\n",
    "        --output_dir temp \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --num_return_sequences 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train DSI-QG with query-represented corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=1 python run.py \\\n",
    "        --task \"DSI\" \\\n",
    "        --model_name \"google/mt5-base\" \\\n",
    "        --run_name \"MSMARCO-100k-mt5-base-DSI-QG\" \\\n",
    "        --max_length 32 \\\n",
    "        --train_file data/msmarco_data/100k/msmarco_corpus.tsv.q10.docTquery \\\n",
    "        --valid_file data/msmarco_data/100k/msmarco_DSI_dev_data.json \\\n",
    "        --output_dir \"models/MSMARCO-100k-mt5-base-DSI-QG\" \\\n",
    "        --learning_rate 0.0005 \\\n",
    "        --warmup_steps 100000 \\\n",
    "        --per_device_train_batch_size 32 \\\n",
    "        --per_device_eval_batch_size 32 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 1000 \\\n",
    "        --max_steps 1000000 \\\n",
    "        --save_strategy steps \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --save_steps 1000 \\\n",
    "        --save_total_limit 2 \\\n",
    "        --load_best_model_at_end \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --dataloader_drop_last False \\\n",
    "        --metric_for_best_model Hits@10 \\\n",
    "        --greater_is_better True \\\n",
    "        --remove_prompt True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-qg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=11.0 -c pytorch\n",
    "# pip install ipywidgets --upgrade\n",
    "# pip install jupyter --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoang/DSI-QG\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-b760cb11643ac61f\n",
      "Reusing dataset json (cache/json/default-b760cb11643ac61f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 91.14it/s]\n",
      "100%|████████████████████████████████| 193020/193020 [00:12<00:00, 14938.55it/s]\n",
      "Using custom data configuration default-de1cdb972ef7496b\n",
      "Reusing dataset json (cache/json/default-de1cdb972ef7496b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 330.89it/s]\n",
      "100%|████████████████████████████████████| 6980/6980 [00:00<00:00, 16097.37it/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 193020\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 582401280\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrunghoang\u001b[0m (\u001b[33mhoangtrung\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/hoang/DSI-QG/wandb/run-20241114_095451-bu1env30\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmsmarco-100k-mt5-base-DSI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtrung/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtrung/huggingface/runs/bu1env30\u001b[0m\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"run.py\", line 194, in <module>\n",
      "    main()\n",
      "  File \"run.py\", line 159, in main\n",
      "    trainer.train()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 2539, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/hoang/DSI-QG/trainer.py\", line 15, in compute_loss\n",
      "    loss = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['labels']).loss\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 1655, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 1023, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 547, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 451, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 413, in forward\n",
      "    attn_weights = nn.functional.dropout(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/functional.py\", line 983, in dropout\n",
      "    else _VF.dropout(input, p, training))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 23.65 GiB total capacity; 2.27 GiB already allocated; 12.00 MiB free; 2.34 GiB reserved in total by PyTorch)\n",
      "Traceback (most recent call last):\n",
      "  File \"run.py\", line 194, in <module>\n",
      "    main()\n",
      "  File \"run.py\", line 159, in main\n",
      "    trainer.train()\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/trainer.py\", line 2539, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/hoang/DSI-QG/trainer.py\", line 15, in compute_loss\n",
      "    loss = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['labels']).loss\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 1655, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 1023, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 547, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 451, in forward\n",
      "    attention_output = self.SelfAttention(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/transformers/models/mt5/modeling_mt5.py\", line 413, in forward\n",
      "    attn_weights = nn.functional.dropout(\n",
      "  File \"/home/hoang/.conda/envs/dsi-qg/lib/python3.8/site-packages/torch/nn/functional.py\", line 983, in dropout\n",
      "    else _VF.dropout(input, p, training))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 23.65 GiB total capacity; 2.27 GiB already allocated; 12.00 MiB free; 2.34 GiB reserved in total by PyTorch)\n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mmsmarco-100k-mt5-base-DSI\u001b[0m at: \u001b[34mhttps://wandb.ai/hoangtrung/huggingface/runs/bu1env30\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_095451-bu1env30/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hoang/DSI-QG\n",
    "!CUDA_VISIBLE_DEVICES=0 python run.py \\\n",
    "        --task \"DSI\" \\\n",
    "        --model_name \"google/mt5-base\" \\\n",
    "        --run_name \"msmarco-100k-mt5-base-DSI\" \\\n",
    "        --max_length 256 \\\n",
    "        --train_file data/msmarco_data/100k/msmarco_DSI_train_data.json \\\n",
    "        --valid_file data/msmarco_data/100k/msmarco_DSI_dev_data.json \\\n",
    "        --output_dir \"models/msmarco-100k-mt5-base-DSI\" \\\n",
    "        --learning_rate 0.0005 \\\n",
    "        --warmup_steps 100 \\\n",
    "        --per_device_train_batch_size 16 \\\n",
    "        --per_device_eval_batch_size 8 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 1000 \\\n",
    "        --max_steps 2000 \\\n",
    "        --save_strategy steps \\\n",
    "        --dataloader_num_workers 10 \\\n",
    "        --save_steps 1000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --load_best_model_at_end \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --report_to wandb \\\n",
    "        --logging_steps 100 \\\n",
    "        --dataloader_drop_last False \\\n",
    "        --metric_for_best_model Hits@10 \\\n",
    "        --greater_is_better True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-qg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
